## 알파
### 1. Spark 도커 컨테이너 생성하여 Kafka에서 데이터 받아 올 환경 구성

![image](https://user-images.githubusercontent.com/66215132/228212543-a7385ed8-29ec-4ca5-b735-ded972927d7b.png)
![image](https://user-images.githubusercontent.com/66215132/228212591-19d66353-2e7d-448b-9e42-b69dcd5ac993.png)

(출처: https://todaycodeplus.tistory.com/31)

- master node, worker1 node, worder2 node, jupyterlab으로 구성
- worker node 개수는 원하는 대로 조절 가능
- 기존에 사용하려고 했던 zeppelin에서는 에러를 확인하기 불편하다는 글을 보고 노트북을 띄우지 않고 spark-submit으로 진행하려고 했었으나 read 방식만 변경하면 처리할 수 있다고 함.
- 따라서 노트북을 띄워 진행하는데, zeppelin보다 더 익숙한 노트북인 jupyter를 사용하기로 함.

### 2. Kafka + Spark 연동 작업

- 로컬에서 Kafka, Spark 서버 같이 띄워서 데이터 받아와지는지 확인 - 진행 중
- 확인되면 이후부터는 AWS 서버에서 진행하려고 함

***

## 수업
- **빅데이터플랫폼**
   - 유튜버 '때껄룩'의 최신 영상 70개 정도와 '때껄룩'의 영상 중 사랑과 관련된 플레이리스트 30개 정도를 선정하여 댓글 크롤링
      - 댓글 크롤링 결과 47865개의 댓글이 수집됨
   - 전처리 & 문장 분류
      - 결측값과 중복값을 제거하고 정규표현식을 통해 한글이 아닌 문자는 삭제 => 42815개의 댓글 데이터
      - 댓글 데이터를 문장 별로 분류하기 위해 kiwi 형태소 분석기 라이브러리에서 제공하는 문장 분리 기능을 사용 => 118882개의 문장으로 분류됨
   - 토큰화 & 워드 임베딩
      - 윈도우 cmd 창에서 KLT2000을 사용해 토큰화
      - 토큰화 한 데이터를 word2vec training을 시켜 모델을 만듦
   - 문장 벡터 생성 & 임의의 문장 유사도 계산
      - 워드 임베딩한 데이터를 통해 수집한 모든 문장에 대해 같은 문장을 이루고 있는 토큰들의 벡터를 모두 더하여 문장 벡터 구성
      - 임의로 정한 문장인 '나 너 좋아해'의 문장 벡터를 구하고 수집한 문장들의 문장 벡터와 모두 각각 코사인 유사도를 연산
