## 알파
- AWS 서버 구축함! EC2에 ssh로 접속해서 실습 진행 중
- spark-base에 추가

```bash
pip3 install git+https://git@github.com/SKTBrain/KoBERT.git@master #코버트 설치
pip3 install mxnet
pip3 install gluonnlp pandas tqdm
pip3 install sentencepiece
pip3 install transformers
pip3 install torch
pip3 install boto3
pip3 install nltk
pip3 install emoji
pip3 intsall soynlp
```

- spark-streaming.py 에 전처리 코드 추가   
    댓글 별로 토큰화 수행해서 BERTDataset으로 변환되는 거 확인   
    → data_comments(comments_df)를 모델로 넘겨야댐
    

```python
import findspark
findspark.init()

from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, pandas_udf, split, from_json, expr
import pandas as pd
import nltk
from nltk.corpus import stopwords
from transformers import AutoTokenizer
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

#kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#regularization
import re
import emoji
from soynlp.normalizer import repeat_normalize

#BERT 모델, Vocabulary 불러오기
bertmodel, vocab = get_pytorch_kobert_model()

max_len = 150
batch_size = 32

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)
        length = len(dataset)
        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

KAFKA_BOOTSTRAP_SERVERS = "broker-1:29092, broker-2:29093, broker-3:29094"
KAFKA_TOPIC = "youtube_comments"

# Create Schema
SCHEMA = StructType([
    StructField("id", StringType(), True),           # COMMENT ID
    StructField("datetime", TimestampType(), True),  # Date and hour
    StructField("user", StringType(), True),         # USER NAME
    StructField("content", StringType(), True),      # CONTENT
])

# Create a Spark Session
spark = SparkSession \
    .builder \
    .appName("kafka-streaming") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

spark.sparkContext.setLogLevel('WARN')

# 정규화
emojis = ''.join(emoji.EMOJI_DATA.keys())
pattern = re.compile(f'[^ .,?!/@$%~％·∼()\x00-\x7Fㄱ-힣{emojis}]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
    x = pattern.sub(' ', x)
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x

# Define the processing function
def process(comments):
    comments = clean(comments)

    comments_tok = pd.DataFrame([[comments, '0']], columns=['comments', 'label'])
    comments_tok.to_csv('comments.tsv', sep='\t', encoding='utf-8', index=False)
    comments_tok = nlp.data.TSVDataset('comments.tsv', field_indices=[0,1], num_discard_samples=1)

    tokenizer = get_tokenizer()
    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)
    # 토큰화
    data_comments = BERTDataset(comments_tok, 0, 1, tok, max_len, True, False)

    comments_df = pd.DataFrame(list(data_comments), columns=['token_ids', 'valid_length', 'segment_ids', 'label'])
    return comments_df.to_string()

# Using Spark Structed Streaming
# Read messages from Kafka Topic and Create Dataframe
df = spark \
    .readStream\
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("failOnDataLoss","False") \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "earliest") \
    .load()

# Apply processing function to the content column
process_udf = udf(lambda z:process(z), StringType())
df_processed = df \
    .select(from_json(col("value").cast("string"), SCHEMA).alias("value")) \
    .selectExpr('value.id', 'value.datetime', 'value.user', 'value.content') \
    .withColumn('content_processed', process_udf(col('content'))) \
    .drop('content') \
    .withColumnRenamed('content_processed', 'content')

# Write processed data to console
df_processed.writeStream \
    .format("console") \
    .option("truncate", "false") \
    .outputMode("append") \
    .start() \
    .awaitTermination()
```

- data_comments는 대충 일케 생김

```
token_ids [2, 2732, 5907, 5760, 2860, 6295, 7096, 2186, 6459, 6844, 7831, 5761, 2044, 7788, 7142, 7318, 633, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]           
valid_length 18  
segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]      
label 0
```

***

## 수업
- **컴퓨터네트워크**   
    - data communications: > 두 개의 디바이스에서 < wire/wireless든 어떠한 형태로 transmission media를 교환하는 것   
      4가지 특징이 있다   
        1. delivery 맞는 곳에 보내준다.
        2. accuracy 데이터가 깨지지 않게 보내준다. without distortion or corruption
        3. timeliness 내가 원하는 시간 안에 보내준다. ***네트워크에서 중요한 개념***
        4. jitter 전자공학 분야.. 음성 데이터와 이미지 데이터가 싱크가 맞게 보내주는 것처럼 패킷 전송 시간에 관한 것   

    - 데이터 통신의 다섯 가지 컴포넌트   
    protocol		                               	protocol   
                              message   
            sender	--transmission medium--  receiver   

        - protocol stack
        - protocol suite 어떤 서비스를 위해 프로토콜을 스태킹 한 것   
          보통 4레이어 transport protocol과 5레이어 application protocol은 짝이 맞음   

        sender==source==transmitter == host, station   
        receiver==destination==target == host, station   
        sender/receiver/중간전달자 == node   
        transmission medium == link==channel==line: wire/wireless   

        sender와 receiver를 엔드포인트라고 불러서 둘 사이의 통신을 end-to-end라고 부름   

        message : text, numbers, images, audio, video... 0과1로 표현되어있는 비트패턴   

    - data flow   
        1.일방 simplex (키보드, 평범한 모니터링 시스템 ...)
        2.양방인데 순서가 있는 half-duplex (워키토키, 무전기 ...)
        3.양방인데 순서가 없는 full-duplex (전화 ...)

    - 네트워크   
    통신 기능이 있는 디바이스들을 연결시켜주는 것   
    디바이스 >   
        컴퓨터와 같은 host   
        데이터의 형태를 변형시키는 라우터, 스위치, 모뎀과 같은 connecting device
        
    네트워크 평가 기준(criteria): 경제성이 중요. low cost high performance   
        1. performance   
            transit time and response time 전송시간, 응답시간   
            throughput 많은 양의 데이터를 보냈을 때 receiver가 얼만큼 성공적으로 받았는가/100개를 보냈을 때 몇 개가 성공적으로 왔는가   
            delay 시간   
        2. reliability   
            frequency of failure. time for recover from a failure. robustness in a catastrophe(견고함).   
        3. security

    - physical structures
        - Type of Connection
            a. point-to-point   
                dedicated link between two devices(전용성)   
                dedicated link를 항상 24시간 쓰진 않기 때문에(비용 문제) -> multipoint 사용    
    
            b. multipoint(multidrop)   
                하나의 single link를 여러 디바이스가 나눠쓰는 것   
    
        - Physical Topology
    1. mesh   
    link 수 계산법: 콤비네이션(5개 노드면 5C2=10)   
    모든 디바이스가 각각 dedicated point-to-point 링크를 가지고 있음   
    장점: 트래픽문제없음/견고함(링크하나망가져도딱히문제없음)/높은프라이버시와보안   
    단점: 설치비용/High cost
        
    2. star   
    허브에 꽂아쓰는 방식. 가장 많이 쓰임   
    단점: 허브가 죽으면 사용 못 함   
        
    3. bus   
    멀티포인트. 멀티탭같은 느낌   
    backbone cable이 설치되면 노드들은 drop line이랑 tap을 통해 연결만 하면 됨   
    단점: 접속된 노드 수만큼 전체적인 capacity가 낮아짐   
    signal reflection 발생   
    a fault or break in bus cable stops all tx   
        
    4. ring   
    각각의 노드들에 토큰이 있음. 토큰이 있는 노드가 네트워크 액세스에 접속할 수 있는 권한이 생김.   
    설치와 재구성이 쉬움   
    일방 전송   
    disable node가 생겨 링이 부서지면 모든 네트워크 사용 불가   
    => 더블링: 링을 두 개 설치   
    outer ring과 inner ring의 토큰 회전 방식도 다르게 만듦   
        

network types

1. size   
2. geographical coverage   
3. ownership   
를 기준으로 나눠짐   

- LAN (Local Area Network)   
    privately owned (건물 안, 학교 내 등 owner 내의 host들만 연결함)   
    LAN 안의 각각의 호스트는 *unique*한 identifier, address를 가짐   
    (ip address: globally unique, 맥 어드레스: locally unique, socket address: 프로세스가 os와 communication할때의 number 컴퓨터 내에서만 unique)   
    1. size : limited size
    2. geographical coverage : interconnect host
    3. ownership (국민대학교 ...) : privately owned
   
- WAN (Wide Area Network)   
    일반적으로 point-to-point WAN   
    switched WAN: Combination of several point-to-point WANs with switches   
    1. size : wider geographical span(town, country, world)
    2. geographical coverage : 스위치나 라우터나 모뎀같은 디바이스를 연결
    3. ownership : communication companies (Telco)

internet(between network)==internetwork   
Internet(인터넷)

***

## 이번 주 할 일
- 컴네 셤 준비
- 빅최기 발표 준비
- 운체 셤 준비
